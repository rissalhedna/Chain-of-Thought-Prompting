{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import rootutils\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_prompts(dataset, client, system_prompt):\n",
    "    \"\"\"\n",
    "    Evaluates different prompt strategies on the provided dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - math_ds: List of dictionaries with 'question' and 'answer' keys.\n",
    "    - client: The client object used by callGPT.\n",
    "    - system_prompt: The system prompt to be used with callGPT.\n",
    "    \n",
    "    Returns:\n",
    "    - metrics: A dictionary with accuracy metrics for each prompt type.\n",
    "    \"\"\"\n",
    "    for example in tqdm(dataset, desc=\"Evaluating Prompts\"):\n",
    "        question = example.get('question', '').strip()\n",
    "        true_answer_raw = example.get('answer', '')\n",
    "        true_answer = clean_answer(extract_answer(true_answer_raw))\n",
    "        \n",
    "        if not question or not true_answer:\n",
    "            continue\n",
    "        \n",
    "        for prompt_name, prompt_func in prompt_functions.items():\n",
    "            try:\n",
    "                prompt = prompt_func(question)\n",
    "                model_output = callGPT(system_prompt, prompt, client)\n",
    "                \n",
    "                model_answer_raw = extract_answer(model_output)\n",
    "                model_answer = clean_answer(model_answer_raw)\n",
    "                \n",
    "                metrics[prompt_name]['total'] += 1\n",
    "                if model_answer == true_answer:\n",
    "                    metrics[prompt_name]['correct'] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error with prompt '{prompt_name}' on question '{question}': {e}\")\n",
    "                continue\n",
    "    \n",
    "    accuracy_results = {}\n",
    "    for prompt_name, data in metrics.items():\n",
    "        if data['total'] > 0:\n",
    "            accuracy = data['correct'] / data['total']\n",
    "            accuracy_results[prompt_name] = accuracy\n",
    "        else:\n",
    "            accuracy_results[prompt_name] = None  # No data for this prompt\n",
    "    \n",
    "    return accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, Any, List\n",
    "current_file = os.getcwd()\n",
    "root = rootutils.find_root(search_from=current_file, indicator=\".project-root\")\n",
    "rootutils.setup_root(root, pythonpath=True)\n",
    "\n",
    "from src.prompt_utils import getKojimaPrompt, getRegularPrompt, MATH_SYSTEM_PROMPT\n",
    "from src.gpt_utils import callGPT\n",
    "\n",
    "prompt_functions = {\n",
    "    'Kojima': getKojimaPrompt,\n",
    "    'Regular': getRegularPrompt,\n",
    "}\n",
    "\n",
    "metrics = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "\n",
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extracts the answer following the '####' delimiter.\n",
    "    If '####' is not found, returns an empty string.\n",
    "    \"\"\"\n",
    "    delimiter = '####'\n",
    "    if delimiter in text:\n",
    "        return text.split(delimiter)[-1].strip()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def clean_answer(answer):\n",
    "    \"\"\"\n",
    "    Cleans the extracted answer for comparison.\n",
    "    This can include stripping whitespace, converting to lowercase, etc.\n",
    "    Modify as needed based on the answer format.\n",
    "    \"\"\"\n",
    "    return answer.strip().lower()\n",
    "\n",
    "def process_example(args: tuple) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single example with all prompt types.\n",
    "    \"\"\"\n",
    "    example, client, system_prompt = args\n",
    "    \n",
    "    question = example.get('question', '').strip()\n",
    "    true_answer_raw = example.get('answer', '')\n",
    "    true_answer = clean_answer(extract_answer(true_answer_raw))\n",
    "    \n",
    "    if not question or not true_answer:\n",
    "        return None\n",
    "    \n",
    "    example_results = {\n",
    "        'question': question,\n",
    "        'true_answer': true_answer_raw,\n",
    "        'prompt_results': {}\n",
    "    }\n",
    "    \n",
    "    for prompt_name, prompt_func in prompt_functions.items():\n",
    "        try:\n",
    "            prompt = prompt_func(question)\n",
    "            model_output = callGPT(system_prompt, prompt, client)\n",
    "            \n",
    "            model_answer_raw = extract_answer(model_output)\n",
    "            model_answer = clean_answer(model_answer_raw)\n",
    "            \n",
    "            example_results['prompt_results'][prompt_name] = {\n",
    "                'prompt': prompt,\n",
    "                'model_output': model_output,\n",
    "                'extracted_answer': model_answer_raw,\n",
    "                'is_correct': model_answer == true_answer\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with prompt '{prompt_name}' on question '{question}': {e}\")\n",
    "            example_results['prompt_results'][prompt_name] = {\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    return example_results\n",
    "\n",
    "def evaluate_prompts(dataset: List[Dict], \n",
    "                    client, \n",
    "                    system_prompt: str,\n",
    "                    output_path: str = \"evaluation_results.json\",\n",
    "                    max_workers: int = 5) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluates different prompt strategies and saves all results to a single JSON file.\n",
    "    \"\"\"\n",
    "    metrics = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "    all_results = {\n",
    "        'metadata': {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_examples': len(dataset)\n",
    "        },\n",
    "        'results': []\n",
    "    }\n",
    "    \n",
    "    # Create arguments for parallel processing\n",
    "    process_args = [(example, client, system_prompt) for example in dataset]\n",
    "    \n",
    "    # Process examples in parallel using a simpler approach\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Use list comprehension with tqdm\n",
    "        futures = [executor.submit(process_example, args) for args in process_args]\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(futures, total=len(dataset), desc=\"Processing examples\"):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    all_results['results'].append(result)\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    for prompt_name, prompt_result in result['prompt_results'].items():\n",
    "                        if 'is_correct' in prompt_result:\n",
    "                            metrics[prompt_name]['total'] += 1\n",
    "                            if prompt_result['is_correct']:\n",
    "                                metrics[prompt_name]['correct'] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing example: {e}\")\n",
    "    \n",
    "    # Calculate final accuracies\n",
    "    accuracy_results = {}\n",
    "    for prompt_name, data in metrics.items():\n",
    "        if data['total'] > 0:\n",
    "            accuracy = data['correct'] / data['total']\n",
    "            accuracy_results[prompt_name] = accuracy\n",
    "        else:\n",
    "            accuracy_results[prompt_name] = None\n",
    "    \n",
    "    # Add metrics to results\n",
    "    all_results['metrics'] = {\n",
    "        'accuracy_results': accuracy_results,\n",
    "        'detailed_metrics': {k: dict(v) for k, v in metrics.items()}\n",
    "    }\n",
    "    \n",
    "    # Save all results to a single JSON file\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {output_path}\")\n",
    "    return accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset 'openai/gsm8k'.\n",
      "Processed 'openai/gsm8k' dataset.\n"
     ]
    }
   ],
   "source": [
    "from src.gpt_utils import initialize_openai_model\n",
    "from src.data_utils import CoTDataset\n",
    "\n",
    "client = initialize_openai_model()\n",
    "\n",
    "dataset = list(CoTDataset(\"openai/gsm8k\"))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing examples: 100%|██████████| 10/10 [00:10<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to: evaluation_results.json\n",
      "\n",
      "Accuracy Results:\n",
      "Kojima: 90.00%\n",
      "Regular: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_prompts(\n",
    "    dataset=dataset,\n",
    "    client=client,\n",
    "    system_prompt=MATH_SYSTEM_PROMPT,\n",
    "    output_path=\"evaluation_results.json\",\n",
    "    max_workers=5\n",
    ")\n",
    "\n",
    "print(\"\\nAccuracy Results:\")\n",
    "for prompt_name, accuracy in results.items():\n",
    "    print(f\"{prompt_name}: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
